base {
    ########## DATA #########
    max_ment_width = 30         # maximal number of words in a single mention
    genres = [bc, bn, mz, nw, pt, tc, wb] # possible genres for a document

    ######### MODEL ########
    bin_widths = [1, 1, 1, 1, 1, 3, 8, 16, 32, 1]   # width of bins for mention distance embedding
    dropout = 0.3               # dropout probability for certain layers
    feature_size = 20           # embedding size for certain features e.g. genre, width etc.
    hidden_size = 3000          # hidden layer size of mention and antecedent scorers
    hidden_depth = 1            # number of hidden layers for several scorers
    max_ment_dist = 250         # maximal distance to antecedent (measured in words)
    ment_ratio = 0.4            # used to calculate top k mention to consider
    max_antes = 50              # max number of antecedents per mention to consider
    coref_depth = 2             # number of iterations of inference procedure

    ####### TRAINING #######
    train_data_file = train.english.jsonlines
    epochs = 20                 # number of epochs to fine-tune bert and train task-model

    ###### EVALUATION ######
    eval_gold_path = ./data/data/test.english.v4_gold_conll
    eval_data_file = test.english.jsonlines
}


bert-base = ${base}{
    ######### BERT #########
    bert = bert-base-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_base
}
bert-base-dev = ${bert-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


bert-large = ${base}{
    ######### BERT #########
    bert = bert-large-cased     # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_large
}
bert-large-dev = ${bert-large}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


spanbert-base = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-base-cased         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 2e-05             # initial learning rate for fine-tuning bert
    lr_task = 1e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_base
}
spanbert-base-dev = ${spanbert-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


spanbert-large = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-large-cased        # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_large
}
spanbert-large-dev = ${spanbert-large}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


###
#    Alternative English Language Models
#    Described in Section 4.3 of the thesis
###

roberta-base = ${base}{
    ######### BERT #########
    bert = roberta-base         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/roberta_base
}
roberta-base-dev = ${roberta-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


distilbert-base = ${base}{
    ######### BERT #########
    bert = distilbert-base-cased                # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 2e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/distilbert_base
}
distilbert-base-dev = ${distilbert-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


distilbert-base-uncased = ${base}{
    ######### BERT #########
    bert = distilbert-base-uncased                # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 2e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/distilbert_base_uncased
}
distilbert-base-uncased-dev = ${distilbert-base-uncased}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


distilroberta-base = ${base}{
    ######### BERT #########
    bert = distilroberta-base                # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/distilroberta_base
}
distilroberta-base-dev = ${distilroberta-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


tinybert-312D = ${base}{
    ######### BERT #########
    bert = huawei-noah/TinyBERT_General_4L_312D         # name of hugging face transformer model
    bert_emb_size = 312         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 2e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/tinybert_312D
}
tinybert-312D-dev = ${tinybert-312D}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


tinybert-768D = ${base}{
    ######### BERT #########
    bert = huawei-noah/TinyBERT_General_6L_768D         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/tinybert_768D
}
tinybert-768D-dev = ${tinybert-768D}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


electra-small = ${base}{
    ######### BERT #########
    bert = google/electra-small-discriminator      # name of hugging face transformer model
    bert_emb_size = 256         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/electra
}
electra-small-dev = ${electra-small}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


electra-base = ${base}{
    ######### BERT #########
    bert = google/electra-base-discriminator      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/electra
}
electra-base-dev = ${electra-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


electra-large = ${base}{
    epochs = 40

    ######### BERT #########
    bert = google/electra-large-discriminator      # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/electra
}
electra-large-dev = ${electra-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    eval_data_file = dev.english.jsonlines
}


###
#    German Language Models tested on TÃ¼Ba-D/Z v10
#    Described in Section 5.3.1 of the thesis
###

german-deepset-bert-base = ${base}{
    epochs = 26

    ######### BERT #########
    bert = bert-base-german-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/german_deepset_bert_base
    train_data_file = train.tuebadz-v10.jsonlines
    eval_data_file = test.tuebadz-v10.jsonlines
    eval_gold_path = ./data/data/test.tuebadz-v10.v4_gold_conll
}
german-deepset-bert-base-dev = ${german-deepset-bert-base}{
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    eval_data_file = dev.tuebadz-v10.jsonlines
}


german-dbmdz-bert-base  = ${base}{
    epochs = 26

    ######### BERT #########
    bert = bert-base-german-dbmdz-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/german_dbmdz_bert_base
    train_data_file = train.tuebadz-v10.jsonlines
    eval_data_file = test.tuebadz-v10.jsonlines
    eval_gold_path = ./data/data/test.tuebadz-v10.v4_gold_conll
}
german-dbmdz-bert-base-dev = ${german-dbmdz-bert-base}{
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    eval_data_file = dev.tuebadz-v10.jsonlines
}


gbert-base = ${base}{
    epochs = 26

    ######### BERT #########
    bert = deepset/gbert-base      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/gbert_base
    train_data_file = train.tuebadz-v10.jsonlines
    eval_data_file = test.tuebadz-v10.jsonlines
    eval_gold_path = ./data/data/test.tuebadz-v10.v4_gold_conll
}
gbert-base-dev = ${gbert-base}{
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    eval_data_file = dev.tuebdaz-v10.jsonlines
}


gng-gelectra-base = ${base}{
    epochs = 40
    # epochs = 20
    # epochs = 63
    # epochs = 200

    ######### BERT #########
    bert = german-nlp-group/electra-base-german-uncased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into
    # segm_size = 128           # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/gng_gelectra_base
    # data_folder = ./data/data/gng_gelectra_base_128
    train_data_file = train.tuebadz-v10.jsonlines
    # train_data_file = train.semeval.jsonlines
    # train_data_file = train.dirndl.jsonlines
    eval_data_file = dev.tuebadz-v10.jsonlines
    # eval_data_file = test.semeval.jsonlines
    # eval_data_file = test.dirndl.jsonlines
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    # eval_gold_path = ./data/data/test.semeval.v4_gold_conll
    # eval_gold_path = ./data/data/test.dirndl.v4_gold_conll
}
gng-gelectra-base-dev = ${gng-gelectra-base}{
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    # eval_gold_path = ./data/data/dev.semeval.v4_gold_conll
    # eval_gold_path = ./data/data/dev.dirndl.v4_gold_conll
    eval_data_file = dev.tuebadz-v10.jsonlines
    # eval_data_file = dev.semeval.jsonlines
    # eval_data_file = dev.dirndl.jsonlines
}


gelectra-base = ${base}{
    epochs = 40

    ######### BERT #########
    bert = deepset/gelectra-base      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/gelectra_base
    train_data_file = train.tuebadz-v10.jsonlines
    eval_data_file = dev.tuebadz-v10.jsonlines
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
}
gelectra-base-dev = ${gelectra-base}{
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    eval_data_file = dev.tuebadz-v10.jsonlines
}


gelectra-large = ${base}{
    epochs = 40

    ######### BERT #########
    bert = deepset/gelectra-large      # name of hugging face transformer model
    bert_emb_size = 1024         # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/gelectra_large
    train_data_file = train.tuebadz-v10.jsonlines
    eval_data_file = dev.tuebadz-v10.jsonlines
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
}
gelectra-large-dev = ${gelectra-large}{
    eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    eval_data_file = dev.tuebadz-v10.jsonlines
}


###
#    Multilingual Language Models for Cross-lingual Coreference Resolution
#    Described in Section 5.3.2 of the thesis
###

bert-multilingual-base = ${base}{
    # epochs = 26
    # epochs = 63
    # epochs = 200

    ######### BERT #########
    bert = bert-base-multilingual-cased         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ##### ADV. TRAINING ####
    adv_data_file = train.tuebadz-v10.jsonlines
    lr_gen = 3e-07
    lr_dis = 2e-04

    ######### PATHS ########
    data_folder = ./data/data/mbert-base-cased
    emb_src_data_file = emb.wmt17_10k_en.jsonlines
    emb_tgt_data_file = emb.wmt17_10k_de.jsonlines
    # train_data_file = train.tuebadz-v10.jsonlines
    # train_data_file = train.semeval.jsonlines
    # train_data_file = train.dirndl.jsonlines
    # eval_data_file = test.tuebadz-v10.jsonlines
    # eval_data_file = test.semeval.jsonlines
    # eval_data_file = test.dirndl.jsonlines
    # eval_gold_path = ./data/data/test.tuebadz-v10.v4_gold_conll
    # eval_gold_path = ./data/data/test.semeval.v4_gold_conll
    # eval_gold_path = ./data/data/test.dirndl.v4_gold_conll
}
bert-multilingual-base-dev = ${bert-multilingual-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    # eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    # eval_gold_path = ./data/data/dev.semeval.v4_gold_conll
    # eval_gold_path = ./data/data/dev.dirndl.v4_gold_conll
    eval_data_file = dev.english.jsonlines
    # eval_data_file = dev.tuebadz-v10.jsonlines
    # eval_data_file = dev.semeval.jsonlines
    # eval_data_file = dev.dirndl.jsonlines
}


xlm-roberta-base = ${base}{
    # epochs = 26
    # epochs = 63
    # epochs = 200

    ######### BERT #########
    bert = xlm-roberta-base                     # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/xlm_roberta_base
    # train_data_file = train.tuebadz-v10.jsonlines
    # train_data_file = train.semeval.jsonlines
    # train_data_file = train.dirndl.jsonlines
    # eval_data_file = test.tuebadz-v10.jsonlines
    # eval_data_file = test.semeval.jsonlines
    # eval_data_file = test.dirndl.jsonlines
    # eval_gold_path = ./data/data/test.tuebadz-v10.v4_gold_conll
    # eval_gold_path = ./data/data/test.semeval.v4_gold_conll
    # eval_gold_path = ./data/data/test.dirndl.v4_gold_conll
}
xlm-roberta-base-dev = ${xlm-roberta-base}{
    eval_gold_path = ./data/data/dev.english.v4_auto_conll
    # eval_gold_path = ./data/data/dev.tuebadz-v10.v4_gold_conll
    # eval_gold_path = ./data/data/dev.semeval.v4_gold_conll
    # eval_gold_path = ./data/data/dev.dirndl.v4_gold_conll
    eval_data_file = dev.english.jsonlines
    # eval_data_file = dev.tuebadz-v10.jsonlines
    # eval_data_file = dev.semeval.jsonlines
    # eval_data_file = dev.dirndl.jsonlines
}