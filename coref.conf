base {
    ########## DATA #########
    cont_emb = ./data/embs/glove.840B.300d.txt.filtered
    head_emb = ./data/embs/glove_50_300_2.txt.filtered
    char_vocab = ./data/embs/char_vocab.english.txt
    cont_size = 300             # size of the context embedding
    head_size = 300             # size of the head embedding
    elmo_size = 1024            # size of elmo embedding
    max_ment_width = 30         # maximal number of words in a single mention
    max_sent_num = 50           # maximal number of sentence per document (applied only for training)
    genres = [bc, bn, mz, nw, pt, tc, wb] # possible genres for a document

    ######### MODEL ########
    bin_widths = [1, 1, 1, 1, 1, 3, 8, 16, 32, 1]   # width of bins for mention distance embedding
    dropout = 0.2               # dropout probability for certain layers
    dropout_lexical = 0.5       # dropout probability for word embeddings
    word_emb = 1474             # context + char + elmo embedding
    feature_size = 20           # embedding size for certain features e.g. genre, width etc.
    hidden_size = 150           # hidden layer size of mention and antecedent scorers
    lstm_hidden_size = 200      # hidden layer size of lstm
    lstm_dropout = 0.4          # dropout probability for hidden states
    max_ment_dist = 250         # maximal distance to antecedent (measured in words)
    ment_ratio = 0.4            # used to calculate top k mention to consider
    max_antes = 50              # max number of antecedents per mention to consider
    coref_depth = 2             # number of iterations of inference procedure
    char_num = 115              # size of character vocabulary
    char_emb = 8                # size of character embedding
    kernel_num = 50             # kernel number of character embedder cnns
    kernel_size = [3, 4, 5]     # kernels sizes of character embedder cnns

    ####### TRAINING #######
    train_data_path = ./data/data/train.english.jsonlines
    train_elmo_path = ./data/embs/train.elmo_cache.hdf5
    decay_rate = 0.999          # factor to multiply learning rate with
    decay_step = 100            # reduce learning rate after this number of steps
    epochs = 179                # just over 500k steps with original training data
    lr=0.001                    # initial learning rate

    ###### EVALUATION ######
    eval_gold_path = ./data/data/test.english.v4_gold_conll
    eval_data_path = ./data/data/test.english.jsonlines
    eval_elmo_path = ./data/embs/test.elmo_cache.hdf5
    # eval_gold_path = ./data/data/dev.english.v4_gold_conll
    # eval_data_path = ./data/data/dev.english.jsonlines
    # eval_elmo_path = ./data/embs/dev.elmo_cache.hdf5
}