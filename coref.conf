base {
    ########## DATA #########
    max_ment_width = 30         # maximal number of words in a single mention
    genres = [bc, bn, mz, nw, pt, tc, wb] # possible genres for a document

    ######### MODEL ########
    bin_widths = [1, 1, 1, 1, 1, 3, 8, 16, 32, 1]   # width of bins for mention distance embedding
    dropout = 0.3               # dropout probability for certain layers
    feature_size = 20           # embedding size for certain features e.g. genre, width etc.
    hidden_size = 3000          # hidden layer size of mention and antecedent scorers
    hidden_depth = 1            # number of hidden layers for several scorers
    max_ment_dist = 250         # maximal distance to antecedent (measured in words)
    ment_ratio = 0.4            # used to calculate top k mention to consider
    max_antes = 50              # max number of antecedents per mention to consider
    coref_depth = 2             # number of iterations of inference procedure

    ####### TRAINING #######
    train_data_file = train.english.jsonlines
    epochs = 20                 # number of epochs to fine-tune bert and train task-model

    ###### EVALUATION ######
    eval_gold_path = ./data/data/test.english.v4_gold_conll
    eval_data_file = test.english.jsonlines
    # eval_gold_path = ./data/data/dev.english.v4_gold_conll
    # eval_data_file = dev.english.jsonlines
}


bert-base = ${base}{
    ######### BERT #########
    bert = bert-base-cased      # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 128             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 11           # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_base
}


bert-large = ${base}{
    ######### BERT #########
    bert = bert-large-cased     # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 2e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/bert_large
}


spanbert-base = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-base-cased         # name of hugging face transformer model
    bert_emb_size = 768         # name of hugging face transformer model
    segm_size = 384             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 2e-05             # initial learning rate for fine-tuning bert
    lr_task = 1e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_base
}


spanbert-large = ${base}{
    ######### BERT #########
    bert = SpanBERT/spanbert-large-cased        # name of hugging face transformer model
    bert_emb_size = 1024        # name of hugging face transformer model
    segm_size = 512             # size of the segments the document is split into

    ####### TRAINING #######
    max_segm_num = 3            # maximal number of segments per document (applied only for training)
    lr_bert = 1e-05             # initial learning rate for fine-tuning bert
    lr_task = 3e-04             # initial learning rate for task specific layers

    ######### PATHS ########
    data_folder = ./data/data/spanbert_large
}